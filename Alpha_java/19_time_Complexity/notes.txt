==================================================
TIME COMPLEXITY â€“ INTERVIEW NOTES
==================================================

ðŸ”¹ 1. What is Time Complexity?
- Time complexity tells us how the runtime of an algorithm grows
  with the size of input (n).
- It helps to compare algorithms without running them.
- Expressed using Big-O Notation.

--------------------------------------------------
ðŸ”¹ 2. Big-O, Big-Î©, and Big-Î˜
- Big-O (O): Worst case time (upper bound).
- Big-Î© (Î©): Best case time (lower bound).
- Big-Î˜ (Î˜): Average case / Tight bound.

ðŸ‘‰ In interviews, most of the time, we talk about **Big-O**.

--------------------------------------------------
ðŸ”¹ 3. Common Time Complexities
- O(1)  â†’ Constant (array access, hashmap get/put)
- O(log n) â†’ Logarithmic (binary search, tree search)
- O(n)  â†’ Linear (linear search, traversing array)
- O(n log n) â†’ Linearithmic (merge sort, quicksort avg)
- O(n^2) â†’ Quadratic (bubble sort, nested loops)
- O(2^n) â†’ Exponential (recursive fibonacci)
- O(n!) â†’ Factorial (traveling salesman brute force)

--------------------------------------------------
ðŸ”¹ 4. Space Complexity
- Measures how much extra memory an algorithm uses.
- Example: 
   - Iterative Fibonacci â†’ O(1) space
   - Recursive Fibonacci â†’ O(n) space (call stack)

--------------------------------------------------
ðŸ”¹ 5. Nested Loops Rule of Thumb
- Single loop â†’ O(n)
- Two nested loops â†’ O(n^2)
- Three nested loops â†’ O(n^3)
- Loop inside log operation â†’ O(n log n)

--------------------------------------------------
ðŸ”¹ 6. Time Complexity of Common Data Structures
- Array Access: O(1)
- Array Search: O(n)
- Linked List Access: O(n)
- Linked List Insert at head: O(1)
- Stack/Queue Push/Pop: O(1)
- HashMap Insert/Search/Delete: O(1) average, O(n) worst
- Binary Search Tree Search: O(log n) (if balanced)

--------------------------------------------------
ðŸ”¹ 7. Sorting Algorithms (Very Important in Interviews)
- Bubble Sort â†’ O(n^2)
- Selection Sort â†’ O(n^2)
- Insertion Sort â†’ O(n^2), but O(n) if nearly sorted
- Merge Sort â†’ O(n log n)
- Quick Sort â†’ O(n log n) avg, O(n^2) worst
- Heap Sort â†’ O(n log n)
- Counting Sort â†’ O(n + k) (works only for limited range)

--------------------------------------------------
ðŸ”¹ 8. Common Interview Questions
Q1: What is better â€“ O(n) or O(log n)?
A: O(log n) is usually better for large n, since it grows much slower.

Q2: Why is quicksort preferred over merge sort in practice?
A: Quicksort is in-place (less memory), faster in practice due to cache efficiency,
   though worst case is O(n^2).

Q3: Difference between time complexity and space complexity?
A: Time â†’ How fast the algorithm runs.
   Space â†’ How much memory it consumes.

Q4: What is amortized analysis?
A: Sometimes operations are cheap most of the time but expensive occasionally.
   Example: Dynamic array (ArrayList in Java) resizing.
   Average per operation â†’ O(1).

Q5: Why is Binary Search O(log n)?
A: Because each step divides the search space into half.

Q6: What is the best time complexity possible for comparison-based sorting?
A: O(n log n). Thatâ€™s why Merge Sort, Heap Sort, Quick Sort are optimal.

--------------------------------------------------
ðŸ”¹ 9. Growth Rate Comparison
For input size n = 10, 100, 1000:

n = 10 â†’ O(n) = 10, O(n^2) = 100
n = 100 â†’ O(n) = 100, O(n^2) = 10,000
n = 1000 â†’ O(n) = 1000, O(n^2) = 1,000,000

ðŸ‘‰ Shows why O(n) is far more scalable than O(n^2).

--------------------------------------------------
ðŸ”¹ 10. Key Takeaways
âœ” Understand the common time complexities.
âœ” Practice calculating complexity of nested loops & recursion.
âœ” Know complexities of data structures & sorting algorithms.
âœ” Remember tradeoffs: Faster time may mean more memory.
âœ” Always justify your answer with reasoning in interviews.

==================================================
END OF NOTES
==================================================
